{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11756206,"sourceType":"datasetVersion","datasetId":7255575},{"sourceId":386452,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":318656,"modelId":338520}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-images-small.tar\n!tar -xf abo-images-small.tar\n!rm abo-images-small.tar","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T07:03:19.887270Z","iopub.execute_input":"2025-05-11T07:03:19.888072Z","iopub.status.idle":"2025-05-11T07:04:51.805753Z","shell.execute_reply.started":"2025-05-11T07:03:19.888035Z","shell.execute_reply":"2025-05-11T07:04:51.804345Z"}},"outputs":[{"name":"stdout","text":"--2025-05-11 07:03:19--  https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-images-small.tar\nResolving amazon-berkeley-objects.s3.amazonaws.com (amazon-berkeley-objects.s3.amazonaws.com)... 16.182.108.241, 3.5.10.150, 3.5.2.206, ...\nConnecting to amazon-berkeley-objects.s3.amazonaws.com (amazon-berkeley-objects.s3.amazonaws.com)|16.182.108.241|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3253381120 (3.0G) [application/x-tar]\nSaving to: ‘abo-images-small.tar’\n\nabo-images-small.ta 100%[===================>]   3.03G  47.2MB/s    in 66s     \n\n2025-05-11 07:04:26 (46.9 MB/s) - ‘abo-images-small.tar’ saved [3253381120/3253381120]\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nimport json\nimport random\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport pickle\nimport shutil\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T07:03:02.734668Z","iopub.execute_input":"2025-05-11T07:03:02.734878Z","iopub.status.idle":"2025-05-11T07:03:02.738971Z","shell.execute_reply.started":"2025-05-11T07:03:02.734861Z","shell.execute_reply":"2025-05-11T07:03:02.738442Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\nwith open(\"/kaggle/input/amazon-berkley-vqa/train.json\", \"r\") as f:\n    questions_data = json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T07:03:02.740290Z","iopub.execute_input":"2025-05-11T07:03:02.740485Z","iopub.status.idle":"2025-05-11T07:03:02.843848Z","shell.execute_reply.started":"2025-05-11T07:03:02.740469Z","shell.execute_reply":"2025-05-11T07:03:02.843213Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"torch.manual_seed(42)\nrandom.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T07:03:02.844573Z","iopub.execute_input":"2025-05-11T07:03:02.844827Z","iopub.status.idle":"2025-05-11T07:03:02.855951Z","shell.execute_reply.started":"2025-05-11T07:03:02.844807Z","shell.execute_reply":"2025-05-11T07:03:02.855287Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"all_entries = []\nfor img_path, qas in list(questions_data.items()):\n    for qa in qas:\n        all_entries.append({\n            \"image_path\": img_path,\n            \"question\": qa[\"question\"],\n            \"answer\": qa[\"answer\"]\n        })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T07:03:02.856781Z","iopub.execute_input":"2025-05-11T07:03:02.857252Z","iopub.status.idle":"2025-05-11T07:03:02.899331Z","shell.execute_reply.started":"2025-05-11T07:03:02.857226Z","shell.execute_reply":"2025-05-11T07:03:02.898788Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_data, val_data = train_test_split(all_entries, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T07:04:55.730837Z","iopub.execute_input":"2025-05-11T07:04:55.731141Z","iopub.status.idle":"2025-05-11T07:04:55.745333Z","shell.execute_reply.started":"2025-05-11T07:04:55.731115Z","shell.execute_reply":"2025-05-11T07:04:55.744805Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class CustomVQADataset(Dataset):\n    def __init__(self, data, processor):\n        self.data = data\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        image = Image.open(item[\"image_path\"]).convert(\"RGB\")\n        question = item[\"question\"]\n        answer = item[\"answer\"].lower()\n\n        # Process the image and question\n        encoding = self.processor(image, question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n        labels = self.processor.tokenizer.encode(answer, max_length=8, padding=\"max_length\", truncation=True, return_tensors='pt')\n\n        encoding[\"labels\"] = labels\n        for k, v in encoding.items():\n            encoding[k] = v.squeeze(0)\n\n        return encoding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T07:04:58.605542Z","iopub.execute_input":"2025-05-11T07:04:58.606071Z","iopub.status.idle":"2025-05-11T07:04:58.611730Z","shell.execute_reply.started":"2025-05-11T07:04:58.606041Z","shell.execute_reply":"2025-05-11T07:04:58.611125Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\nbase_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n# Apply LoRA\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"query\", \"value\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n)\nmodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T07:05:00.984460Z","iopub.execute_input":"2025-05-11T07:05:00.985212Z","iopub.status.idle":"2025-05-11T07:05:09.990292Z","shell.execute_reply.started":"2025-05-11T07:05:00.985186Z","shell.execute_reply":"2025-05-11T07:05:09.989614Z"}},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c54b4d7cabc84e91b25c12a0056e888b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77c516c9902e47f18394d47b286b1523"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c29e0249b08e4278929c30758c090c81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"819eff8131c94c8cbafe186c5b205211"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92fc0ba681f44e9a92f4212e38609924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00cdd378709340829d779877167d247e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdf4c9f0a2d34ea49f410c103a645be2"}},"metadata":{}},{"name":"stdout","text":"trainable params: 1,179,648 || all params: 385,852,220 || trainable%: 0.3057\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"train_dataset = CustomVQADataset(train_data, processor)\nval_dataset = CustomVQADataset(val_data, processor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=12, shuffle=True, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=12, shuffle=False, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T07:05:12.014934Z","iopub.execute_input":"2025-05-11T07:05:12.015749Z","iopub.status.idle":"2025-05-11T07:05:12.020149Z","shell.execute_reply.started":"2025-05-11T07:05:12.015722Z","shell.execute_reply":"2025-05-11T07:05:12.019411Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = torch.nn.DataParallel(model)\nmodel.to(device)\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\nscaler = torch.cuda.amp.GradScaler()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T07:05:13.537595Z","iopub.execute_input":"2025-05-11T07:05:13.537897Z","iopub.status.idle":"2025-05-11T07:05:14.462365Z","shell.execute_reply.started":"2025-05-11T07:05:13.537873Z","shell.execute_reply":"2025-05-11T07:05:14.461425Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3044266455.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"num_epochs = 30\npatience = 3\nmin_eval_loss = float(\"inf\")\nearly_stopping = 0\ntracking = []\n\nfor epoch in range(1, num_epochs):\n    model.train()\n    train_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n        input_ids = batch['input_ids'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n        with torch.autocast(device_type='cuda', dtype=torch.float16):\n            outputs = model(input_ids=input_ids,\n                            pixel_values=pixel_values,\n                            attention_mask=attention_mask,\n                            labels=labels)\n        loss = outputs.loss\n        if loss.ndim > 0:\n            loss = loss.mean() \n        train_loss += loss.item()\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n    # Validation\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\"):\n            input_ids = batch['input_ids'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            with torch.autocast(device_type='cuda', dtype=torch.float16):\n                outputs = model(input_ids=input_ids,\n                                pixel_values=pixel_values,\n                                attention_mask=attention_mask,\n                                labels=labels)\n            loss = outputs.loss\n            if loss.ndim > 0:\n                loss = loss.mean() \n\n            val_loss += loss.item()\n\n    train_loss /= len(train_loader)\n    val_loss /= len(val_loader)\n    tracking.append((train_loss, val_loss))\n\n    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n\n    # Early stopping\n    if val_loss < min_eval_loss:\n        min_eval_loss = val_loss\n        early_stopping = 0\n        model.module.save_pretrained(\"blip-lora-vqa\")\n        shutil.make_archive(\"blip-lora-vqa\", 'zip', \"blip-lora-vqa\")\n        print(\"Saved best model.\")\n    else:\n        early_stopping += 1\n        if early_stopping >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n    scheduler.step()\n\nwith open(\"training_tracking.pkl\", \"wb\") as f:\n    pickle.dump(tracking, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T08:02:38.571379Z","iopub.execute_input":"2025-05-11T08:02:38.571663Z","iopub.status.idle":"2025-05-11T13:47:38.809555Z","shell.execute_reply.started":"2025-05-11T08:02:38.571640Z","shell.execute_reply":"2025-05-11T13:47:38.808124Z"}},"outputs":[{"name":"stderr","text":"Epoch 2 Training: 100%|██████████| 2035/2035 [33:20<00:00,  1.02it/s]\nValidating: 100%|██████████| 509/509 [04:53<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 6.2358 | Val Loss: 6.2116\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|██████████| 2035/2035 [33:21<00:00,  1.02it/s]\nValidating: 100%|██████████| 509/509 [04:54<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 6.1939 | Val Loss: 6.1959\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Training: 100%|██████████| 2035/2035 [33:17<00:00,  1.02it/s]\nValidating: 100%|██████████| 509/509 [04:52<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 6.1755 | Val Loss: 6.1905\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 Training: 100%|██████████| 2035/2035 [33:20<00:00,  1.02it/s]\nValidating: 100%|██████████| 509/509 [04:53<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 6.1629 | Val Loss: 6.1859\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 Training: 100%|██████████| 2035/2035 [33:16<00:00,  1.02it/s]\nValidating: 100%|██████████| 509/509 [04:53<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 6.1531 | Val Loss: 6.1846\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 Training: 100%|██████████| 2035/2035 [33:19<00:00,  1.02it/s]\nValidating: 100%|██████████| 509/509 [04:53<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 6.1447 | Val Loss: 6.1826\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 Training: 100%|██████████| 2035/2035 [33:17<00:00,  1.02it/s]\nValidating: 100%|██████████| 509/509 [04:54<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Train Loss: 6.1375 | Val Loss: 6.1820\nSaved best model.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 Training: 100%|██████████| 2035/2035 [33:13<00:00,  1.02it/s]\nValidating: 100%|██████████| 509/509 [04:53<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Train Loss: 6.1312 | Val Loss: 6.1834\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10 Training: 100%|██████████| 2035/2035 [33:13<00:00,  1.02it/s]\nValidating: 100%|██████████| 509/509 [04:52<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 6.1261 | Val Loss: 6.1821\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11 Training:   4%|▍         | 77/2035 [01:15<32:01,  1.02it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2616035670.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pixel_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/1035136874.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Process the image and question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m                 method).\n\u001b[1;32m   2653\u001b[0m         \"\"\"\n\u001b[0;32m-> 2654\u001b[0;31m         encoded_inputs = self.encode_plus(\n\u001b[0m\u001b[1;32m   2655\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3071\u001b[0m         )\n\u001b[1;32m   3072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3073\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   3074\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m     ) -> BatchEncoding:\n\u001b[1;32m    612\u001b[0m         \u001b[0mbatched_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         batched_output = self._batch_encode_plus(\n\u001b[0m\u001b[1;32m    614\u001b[0m             \u001b[0mbatched_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msanitized_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eventual_warn_about_too_long_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitized_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitized_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     def _encode_plus(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m                     \u001b[0;31m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mas_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtensor_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJAX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"model.module.save_pretrained(\"blip-lora-vqa\")\nprint(\"Saved best model.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T07:51:17.146861Z","iopub.execute_input":"2025-05-11T07:51:17.147703Z","iopub.status.idle":"2025-05-11T07:51:17.581012Z","shell.execute_reply.started":"2025-05-11T07:51:17.147674Z","shell.execute_reply":"2025-05-11T07:51:17.580325Z"}},"outputs":[{"name":"stdout","text":"Saved best model.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from transformers import BlipForQuestionAnswering, BlipProcessor\nfrom peft import PeftModel, PeftConfig\nimport torch\nfrom tqdm import tqdm\nimport torch.nn.functional as F\n\n# Load processor and base model\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\nbase_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n# Load LoRA config and wrap model\npeft_model = PeftModel.from_pretrained(base_model, \"blip-lora-vqa\")\npeft_model.eval()\npeft_model.to(device)\n\n# Exact match evaluation\ntotal = 0\ncorrect = 0\n\nwith torch.no_grad():\n    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n\n        outputs = peft_model.generate(\n            input_ids=input_ids,\n            pixel_values=pixel_values,\n            attention_mask=attention_mask,\n            max_new_tokens=10\n        )\n\n        predictions = processor.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        gold_answers = processor.tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n\n        for pred, gold in zip(predictions, gold_answers):\n            total += 1\n            if pred.strip().lower() == gold.strip().lower():\n                correct += 1\n\naccuracy = correct / total\nprint(f\"Exact Match Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:48:58.376842Z","iopub.execute_input":"2025-05-11T13:48:58.377542Z","iopub.status.idle":"2025-05-11T14:02:22.460995Z","shell.execute_reply.started":"2025-05-11T13:48:58.377517Z","shell.execute_reply":"2025-05-11T14:02:22.460179Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 509/509 [13:22<00:00,  1.58s/it]","output_type":"stream"},{"name":"stdout","text":"Exact Match Accuracy: 0.2950\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"for i in range(len(predictions)):\n    print(predictions[i], gold_answers[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:06:17.145251Z","iopub.execute_input":"2025-05-11T14:06:17.146006Z","iopub.status.idle":"2025-05-11T14:06:17.150628Z","shell.execute_reply.started":"2025-05-11T14:06:17.145982Z","shell.execute_reply":"2025-05-11T14:06:17.149815Z"}},"outputs":[{"name":"stdout","text":"yes yes\nyes yes\nblue blue\nbeige yellow\nyes yes\nten amazon\nbeige green\nfabric fabric\nhandmade chair\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"!zip -r blip-lora-vqa.zip blip-lora-vqa/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T07:51:36.546650Z","iopub.execute_input":"2025-05-11T07:51:36.547328Z","iopub.status.idle":"2025-05-11T07:51:36.959753Z","shell.execute_reply.started":"2025-05-11T07:51:36.547301Z","shell.execute_reply":"2025-05-11T07:51:36.958754Z"}},"outputs":[{"name":"stdout","text":"  adding: blip-lora-vqa/ (stored 0%)\n  adding: blip-lora-vqa/README.md (deflated 66%)\n  adding: blip-lora-vqa/adapter_config.json (deflated 54%)\n  adding: blip-lora-vqa/adapter_model.safetensors","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 7%)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"!ls /kaggle/input/blip-epoch-10/pytorch/test/1/blip-lora-vqa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T06:47:58.730322Z","iopub.execute_input":"2025-05-11T06:47:58.730752Z","iopub.status.idle":"2025-05-11T06:47:59.313122Z","shell.execute_reply.started":"2025-05-11T06:47:58.730719Z","shell.execute_reply":"2025-05-11T06:47:59.311923Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"adapter_config.json  adapter_model.safetensors\tREADME.md\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}